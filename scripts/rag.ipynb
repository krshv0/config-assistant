{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151cd837",
   "metadata": {},
   "source": [
    "# Config-Assistant RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee376181",
   "metadata": {},
   "source": [
    "**1. Intialising Tracers and LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "366607ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_together import ChatTogether, TogetherEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "from pathlib import Path\n",
    "import json \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "langsmith_project = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "langchain_tracking = os.getenv(\"LANGCHAIN_TRACKING\")\n",
    "together_api_key = os.getenv(\"TOGETHER_API_KEY\")\n",
    "langchain_endpoint = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "\n",
    "llm = ChatTogether(\n",
    "    model = \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    together_api_key= os.getenv(\"TOGETHER_API_KEY\"), # type: ignore\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af388b49",
   "metadata": {},
   "source": [
    "**2. Declaring Funcion to Vectorise Dofiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff54171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise_configs(path: Path):\n",
    "    documents = []\n",
    "\n",
    "    for file in tqdm(list(path.rglob(\"*.json\")), desc=\"Processing JSON files\"):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for block in data.get(\"structure\", {}).values():\n",
    "            chunk_text = f\"{block.get('purpose', '')}\\n\\n{block.get('content', '')}\"\n",
    "\n",
    "            raw_metadata = {\n",
    "                \"repo_name\": data.get(\"repo_name\"),\n",
    "                \"path\": block.get(\"path\"),\n",
    "                \"language\": block.get(\"language\"),\n",
    "                \"dependencies\": block.get(\"dependencies\"),\n",
    "            }\n",
    "\n",
    "            # Simple manual filter:\n",
    "            allowed_types = (str, int, float, bool, type(None))\n",
    "            safe_metadata = {k: v for k, v in raw_metadata.items() if isinstance(v, allowed_types)}\n",
    "\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata=safe_metadata,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    embeddings = TogetherEmbeddings(\n",
    "        model=\"togethercomputer/m2-bert-80M-32k-retrieval\",\n",
    "        api_key=os.getenv(\"TOGETHER_API_KEY\")\n",
    "    )\n",
    "\n",
    "    persist_dir = \"/Users/krishiv/Desktop/Projects/config-assistant/retrievers/configs\"\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents,\n",
    "        embeddings,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "\n",
    "    print(f\"[+] Stored {len(documents)} documents in vectorstore at: {persist_dir}\")\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e241c8",
   "metadata": {},
   "source": [
    "**3. Declaring Funtion to Vectorise Docs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "898e5e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise_docs(path):\n",
    "    documents = []\n",
    "    for file in tqdm(list(path.rglob(\"*.json\")), desc=\"Processing JSON files\"):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Use the correct key: \"sections\"\n",
    "        chunk_text = \"\"\n",
    "        for block in data.get(\"sections\", []):\n",
    "            if block.get(\"type\") == \"header\":\n",
    "                if chunk_text:\n",
    "                    documents.append(Document(\n",
    "                        page_content=chunk_text.strip(),\n",
    "                        metadata={\n",
    "                            \"page\": data.get(\"page\", ''),\n",
    "                            \"url\": data.get(\"url\", '')\n",
    "                        }\n",
    "                    ))\n",
    "                    chunk_text = \"\"\n",
    "                chunk_text += f\"{block.get('content', '')}\\n\\n\"\n",
    "            elif block.get(\"type\") in [\"paragraph\", \"list\", \"code\"]:\n",
    "                content = block.get(\"items\", []) if block.get(\"type\") == \"list\" else block.get('content', '')\n",
    "                chunk_text += f\"{content}\\n\\n\"\n",
    "            elif block.get(\"type\") == \"table\":\n",
    "                headers = block.get(\"headers\", [])\n",
    "                rows = block.get(\"rows\", [])\n",
    "                header_line = \"| \" + \" | \".join(headers) + \" |\"\n",
    "                separator_line = \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\"\n",
    "                row_lines = []\n",
    "                for row in rows:\n",
    "                    row_line = \"| \" + \" | \".join(row.get(header, \"\") for header in headers) + \" |\"\n",
    "                    row_lines.append(row_line)\n",
    "                chunk_text += \"\\n\".join([header_line, separator_line] + row_lines) + \"\\n\\n\"\n",
    "        if chunk_text:\n",
    "            documents.append(Document(\n",
    "                page_content=chunk_text.strip(),\n",
    "                metadata={\n",
    "                    \"page\": data.get(\"page\", ''),\n",
    "                    \"url\": data.get(\"url\", '')\n",
    "                }\n",
    "            ))\n",
    "\n",
    "    print(f\"Number of documents to store: {len(documents)}\")\n",
    "    if not documents:\n",
    "        raise ValueError(\"No documents found to vectorise. Check your input data.\")\n",
    "\n",
    "    embeddings = TogetherEmbeddings(\n",
    "        model=\"togethercomputer/m2-bert-80M-32k-retrieval\",\n",
    "        api_key=os.getenv(\"TOGETHER_API_KEY\")\n",
    "    )\n",
    "\n",
    "    persist_dir = \"/Users/krishiv/Desktop/Projects/config-assistant/retrievers/docs\"\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents,\n",
    "        embeddings,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "\n",
    "    print(f\"[+] Stored {len(documents)} documents in vectorstore at: {persist_dir}\")\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52814e4",
   "metadata": {},
   "source": [
    "**4. Setting Up Retriver for Docs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f7d275c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files: 100%|██████████| 12/12 [00:00<00:00, 2396.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents to store: 83\n",
      "[+] Stored 83 documents in vectorstore at: /Users/krishiv/Desktop/Projects/config-assistant/retrievers/docs\n"
     ]
    }
   ],
   "source": [
    "docs_path = Path(\"/Users/krishiv/Desktop/Projects/config-assistant/data/docs\")\n",
    "vectorstore_docs = vectorise_docs(docs_path)\n",
    "retriever_docs = vectorstore_docs.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e3860",
   "metadata": {},
   "source": [
    "**5. Setting Up Retreiver for Configs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "069e298c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files: 100%|██████████| 16/16 [00:00<00:00, 475.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Stored 388 documents in vectorstore at: /Users/krishiv/Desktop/Projects/config-assistant/retrievers/configs\n"
     ]
    }
   ],
   "source": [
    "configs_path = Path(\"/Users/krishiv/Desktop/Projects/config-assistant/data/configs\")\n",
    "vectorstore_configs = vectorise_configs(configs_path)\n",
    "retriever_configs = vectorstore_configs.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61856b62",
   "metadata": {},
   "source": [
    "**6. Docs Retrieval Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3791529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "def retrieve_docs(query: str):\n",
    "    template_docs = \"\"\"\n",
    "    You are a helpful assistant tasked with helping users configure SketchyBar on macOS.\n",
    "\n",
    "    You will be given a USER QUERY about some aspect of configuring SketchyBar.\n",
    "\n",
    "    Your job is to rewrite this query into a *precise search query*, optimized for retrieving the most relevant DOCUMENTATION chunks from the SketchyBar documentation vector store.\n",
    "\n",
    "    For reference, the following are some hihg-level fields of the SketchyBar documentation:\n",
    "    - animations \n",
    "    - bar\n",
    "    - components\n",
    "    - events\n",
    "    - features\n",
    "    - items\n",
    "    - popups\n",
    "    - plugins \n",
    "    - scripting\n",
    "    - setup\n",
    "    - tricks\n",
    "    - types\n",
    "    - querying\n",
    "    - reloading \n",
    "\n",
    "    Guidelines:\n",
    "    - Focus on rewriting to match the style and phrasing of the SketchyBar docs.\n",
    "    - If the user asks about a feature, turn it into a direct documentation topic.\n",
    "    - Do not include user greetings, personal statements, or conversational fluff — only the technical search query.\n",
    "    - Output a single line, no extra text.\n",
    "\n",
    "    USER QUERY: {query}\n",
    "\n",
    "    DOCUMENTATION SEARCH QUERY:\n",
    "    \"\"\"\n",
    "    rewriting_template =  ChatPromptTemplate.from_template(template_docs)\n",
    "\n",
    "\n",
    "    rewritten_query = (\n",
    "        rewriting_template\n",
    "        | ChatTogether(temperature=0)\n",
    "        | (lambda x: x.content if hasattr(x, \"content\") else str(x))\n",
    "    )\n",
    "\n",
    "    rewritten_query_result = rewritten_query.invoke({\"query\": query})\n",
    "\n",
    "    # Use .content if needed\n",
    "    retrieved_docs = retriever_docs.invoke(\n",
    "        rewritten_query_result.content if hasattr(rewritten_query_result, \"content\") else rewritten_query_result\n",
    "    )\n",
    "\n",
    "    print(f\"Rewritten query: {rewritten_query_result.content.strip() if hasattr(rewritten_query_result, 'content') else str(rewritten_query_result).strip()}\")\n",
    "    print(f\"Retrieved {len(retrieved_docs)} docs:\")\n",
    "    for doc in retrieved_docs:\n",
    "        print(doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ran\n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "\n",
    "def retrieve_configs(query: str):\n",
    "    template_configs = \"\"\"\n",
    "You are a helpful assistant tasked with helping users configure SketchyBar on macOS.\n",
    "\n",
    "You will be given a USER QUERY asking for some feature or behavior in SketchyBar.\n",
    "\n",
    "Your task is to generate 5 diverse and precise search queries that will help retrieve the most relevant documents from a vector store of community-contributed SketchyBar configuration files (dotfiles).\n",
    "\n",
    "Guidelines:\n",
    "- Each search query should be clear, concise, and optimized for finding matching configuration code.\n",
    "- Try different angles: vary the wording, include relevant SketchyBar terms, possible item names, script types, or config variables.\n",
    "- Think about common patterns used in SketchyBar config files (such as `sketchybarrc`, `.sh` files, scripting hooks, item setup, animations, etc.).\n",
    "- Do NOT include user greetings, conversational fluff, or explanations — only the 5 search queries as separate lines.\n",
    "\n",
    "Example USER QUERY:\n",
    "\"How can I add a Spotify player with album art to my SketchyBar?\"\n",
    "\n",
    "OUTPUT:\n",
    "spotify player sketchybar config\n",
    "spotify now playing sketchybarrc\n",
    "sketchybar media controls config\n",
    "spotify album art sketchybar item\n",
    "sketchybar integrate spotify script\n",
    "\n",
    "Now for the actual query:\n",
    "\n",
    "USER QUERY: {query}\n",
    "\n",
    "OUTPUT (5 search queries):\n",
    "\"\"\"\n",
    "\n",
    "    multi_query_template = ChatPromptTemplate.from_template(template_configs)  \n",
    "\n",
    "    generate_queries = (\n",
    "        multi_query_template \n",
    "        | ChatTogether(temperature=0) \n",
    "        | (lambda x: x.content if hasattr(x, \"content\") else str(x))\n",
    "        | (lambda x: x.split(\"\\n\"))\n",
    "    )\n",
    "\n",
    "\n",
    "    configs_retrieval_chain = generate_queries | retriever_configs.map() | reciprocal_rank_fusion\n",
    "    retrieved_configs = configs_retrieval_chain.invoke({\"query\": query })\n",
    "    return retrieved_configs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
